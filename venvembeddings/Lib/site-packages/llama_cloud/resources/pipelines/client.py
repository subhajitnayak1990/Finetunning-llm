# This file was auto-generated by Fern from our API Definition.

import typing
import urllib.parse
from json.decoder import JSONDecodeError

from ...core.api_error import ApiError
from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.jsonable_encoder import jsonable_encoder
from ...core.remove_none_from_dict import remove_none_from_dict
from ...errors.unprocessable_entity_error import UnprocessableEntityError
from ...types.chat_data import ChatData
from ...types.cloud_document import CloudDocument
from ...types.cloud_document_create import CloudDocumentCreate
from ...types.data_sink_create import DataSinkCreate
from ...types.http_validation_error import HttpValidationError
from ...types.input_message import InputMessage
from ...types.llama_parse_parameters import LlamaParseParameters
from ...types.managed_ingestion_status_response import ManagedIngestionStatusResponse
from ...types.metadata_filters import MetadataFilters
from ...types.paginated_list_cloud_documents_response import PaginatedListCloudDocumentsResponse
from ...types.pipeline import Pipeline
from ...types.pipeline_create import PipelineCreate
from ...types.pipeline_metadata_config import PipelineMetadataConfig
from ...types.pipeline_type import PipelineType
from ...types.playground_session import PlaygroundSession
from ...types.preset_retrieval_params import PresetRetrievalParams
from ...types.retrieval_mode import RetrievalMode
from ...types.retrieve_results import RetrieveResults
from ...types.sparse_model_config import SparseModelConfig
from ...types.text_node import TextNode
from .types.list_pipeline_documents_api_v_1_pipelines_pipeline_id_documents_get_request_status_refresh_policy import (
    ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy,
)
from .types.paginated_list_pipeline_documents_api_v_1_pipelines_pipeline_id_documents_paginated_get_request_status_refresh_policy import (
    PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy,
)
from .types.pipeline_update_embedding_config import PipelineUpdateEmbeddingConfig
from .types.pipeline_update_transform_config import PipelineUpdateTransformConfig
from .types.retrieval_params_search_filters_inference_schema_value import (
    RetrievalParamsSearchFiltersInferenceSchemaValue,
)

try:
    import pydantic
    if pydantic.__version__.startswith("1."):
        raise ImportError
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class PipelinesClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def search_pipelines(
        self,
        *,
        project_id: typing.Optional[str] = None,
        project_name: typing.Optional[str] = None,
        pipeline_name: typing.Optional[str] = None,
        pipeline_type: typing.Optional[PipelineType] = None,
        organization_id: typing.Optional[str] = None,
    ) -> typing.List[Pipeline]:
        """
        Search for pipelines by various parameters.

        Parameters:
            - project_id: typing.Optional[str].

            - project_name: typing.Optional[str].

            - pipeline_name: typing.Optional[str].

            - pipeline_type: typing.Optional[PipelineType].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import PipelineType
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.search_pipelines(
            pipeline_type=PipelineType.PLAYGROUND,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict(
                {
                    "project_id": project_id,
                    "project_name": project_name,
                    "pipeline_name": pipeline_name,
                    "pipeline_type": pipeline_type,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[Pipeline], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_pipeline(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request: PipelineCreate,
    ) -> Pipeline:
        """
        Create a new pipeline for a project.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - request: PipelineCreate.
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_pipeline(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request: PipelineCreate,
    ) -> Pipeline:
        """
        Upsert a pipeline for a project.
        Updates if a pipeline with the same name and project_id already exists. Otherwise, creates a new pipeline.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - request: PipelineCreate.
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Get a pipeline by ID for a given project.

        Parameters:
            - pipeline_id: str.
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_existing_pipeline(
        self,
        pipeline_id: str,
        *,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        metadata_config: typing.Optional[PipelineMetadataConfig] = OMIT,
        name: typing.Optional[str] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        sparse_model_config: typing.Optional[SparseModelConfig] = OMIT,
        status: typing.Optional[str] = OMIT,
        transform_config: typing.Optional[PipelineUpdateTransformConfig] = OMIT,
    ) -> Pipeline:
        """
        Update an existing pipeline for a project.

        Parameters:
            - pipeline_id: str.

            - data_sink: typing.Optional[DataSinkCreate].

            - data_sink_id: typing.Optional[str].

            - embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig].

            - embedding_model_config_id: typing.Optional[str].

            - llama_parse_parameters: typing.Optional[LlamaParseParameters].

            - managed_pipeline_id: typing.Optional[str].

            - metadata_config: typing.Optional[PipelineMetadataConfig].

            - name: typing.Optional[str].

            - preset_retrieval_parameters: typing.Optional[PresetRetrievalParams].

            - sparse_model_config: typing.Optional[SparseModelConfig].

            - status: typing.Optional[str].

            - transform_config: typing.Optional[PipelineUpdateTransformConfig]. Configuration for the transformation.
        """
        _request: typing.Dict[str, typing.Any] = {}
        if data_sink is not OMIT:
            _request["data_sink"] = data_sink
        if data_sink_id is not OMIT:
            _request["data_sink_id"] = data_sink_id
        if embedding_config is not OMIT:
            _request["embedding_config"] = embedding_config
        if embedding_model_config_id is not OMIT:
            _request["embedding_model_config_id"] = embedding_model_config_id
        if llama_parse_parameters is not OMIT:
            _request["llama_parse_parameters"] = llama_parse_parameters
        if managed_pipeline_id is not OMIT:
            _request["managed_pipeline_id"] = managed_pipeline_id
        if metadata_config is not OMIT:
            _request["metadata_config"] = metadata_config
        if name is not OMIT:
            _request["name"] = name
        if preset_retrieval_parameters is not OMIT:
            _request["preset_retrieval_parameters"] = preset_retrieval_parameters
        if sparse_model_config is not OMIT:
            _request["sparse_model_config"] = sparse_model_config
        if status is not OMIT:
            _request["status"] = status
        if transform_config is not OMIT:
            _request["transform_config"] = transform_config
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline(self, pipeline_id: str) -> None:
        """
        Delete a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def chat(
        self,
        pipeline_id: str,
        *,
        class_name: typing.Optional[str] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        messages: typing.Optional[typing.List[InputMessage]] = OMIT,
    ) -> typing.Any:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters:
            - pipeline_id: str.

            - class_name: typing.Optional[str].

            - data: typing.Optional[ChatData].

            - messages: typing.Optional[typing.List[InputMessage]].
        ---
        from llama_cloud import (
            ChatData,
            FilterCondition,
            LlmParameters,
            MetadataFilters,
            PresetRetrievalParams,
            RetrievalMode,
            SupportedLlmModelNames,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.chat(
            pipeline_id="string",
            data=ChatData(
                llm_parameters=LlmParameters(
                    model_name=SupportedLlmModelNames.GPT_4_O,
                ),
                retrieval_parameters=PresetRetrievalParams(
                    retrieval_mode=RetrievalMode.CHUNKS,
                    search_filters=MetadataFilters(
                        condition=FilterCondition.AND,
                        filters=[],
                    ),
                ),
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if class_name is not OMIT:
            _request["class_name"] = class_name
        if data is not OMIT:
            _request["data"] = data
        if messages is not OMIT:
            _request["messages"] = messages
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/chat"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def copy_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Copy a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/copy"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        only_api_data_source_documents: typing.Optional[bool] = None,
        status_refresh_policy: typing.Optional[
            ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy
        ] = None,
    ) -> typing.List[CloudDocument]:
        """
        Return a list of documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - skip: typing.Optional[int].

            - limit: typing.Optional[int].

            - file_id: typing.Optional[str].

            - only_direct_upload: typing.Optional[bool].

            - only_api_data_source_documents: typing.Optional[bool].

            - status_refresh_policy: typing.Optional[ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy].
        ---
        from llama_cloud import (
            ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_documents(
            pipeline_id="string",
            status_refresh_policy=ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy.CACHED,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            params=remove_none_from_dict(
                {
                    "skip": skip,
                    "limit": limit,
                    "file_id": file_id,
                    "only_direct_upload": only_direct_upload,
                    "only_api_data_source_documents": only_api_data_source_documents,
                    "status_refresh_policy": status_refresh_policy,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_batch_pipeline_documents(
        self, pipeline_id: str, *, request: typing.List[CloudDocumentCreate]
    ) -> typing.List[CloudDocument]:
        """
        Batch create documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[CloudDocumentCreate].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.create_batch_pipeline_documents(
            pipeline_id="string",
            request=[],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_batch_pipeline_documents(
        self, pipeline_id: str, *, request: typing.List[CloudDocumentCreate]
    ) -> typing.List[CloudDocument]:
        """
        Batch create or update a document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[CloudDocumentCreate].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.upsert_batch_pipeline_documents(
            pipeline_id="string",
            request=[],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def paginated_list_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        only_api_data_source_documents: typing.Optional[bool] = None,
        status_refresh_policy: typing.Optional[
            PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy
        ] = None,
    ) -> PaginatedListCloudDocumentsResponse:
        """
        Return a list of documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - skip: typing.Optional[int].

            - limit: typing.Optional[int].

            - file_id: typing.Optional[str].

            - only_direct_upload: typing.Optional[bool].

            - only_api_data_source_documents: typing.Optional[bool].

            - status_refresh_policy: typing.Optional[PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy].
        ---
        from llama_cloud import (
            PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.paginated_list_pipeline_documents(
            pipeline_id="string",
            status_refresh_policy=PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy.CACHED,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/paginated"
            ),
            params=remove_none_from_dict(
                {
                    "skip": skip,
                    "limit": limit,
                    "file_id": file_id,
                    "only_direct_upload": only_direct_upload,
                    "only_api_data_source_documents": only_api_data_source_documents,
                    "status_refresh_policy": status_refresh_policy,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedListCloudDocumentsResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_document(self, document_id: str, pipeline_id: str) -> CloudDocument:
        """
        Return a single document for a pipeline.

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_document(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/{document_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(CloudDocument, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_document(self, document_id: str, pipeline_id: str) -> None:
        """
        Delete a document from a pipeline.
        Initiates an async job that will:

        1. Delete vectors from the vector store
        2. Delete the document from MongoDB after vectors are successfully deleted

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_document(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/{document_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_document_chunks(self, document_id: str, pipeline_id: str) -> typing.List[TextNode]:
        """
        Return a list of chunks for a pipeline document.

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_document_chunks(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/chunks",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[TextNode], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_document_status(self, document_id: str, pipeline_id: str) -> ManagedIngestionStatusResponse:
        """
        Return a single document for a pipeline.

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_document_status(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/status",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def sync_pipeline_document(self, document_id: str, pipeline_id: str) -> typing.Any:
        """
        Sync a specific document for a pipeline.

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.sync_pipeline_document(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/sync",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def force_delete_pipeline(self, pipeline_id: str) -> None:
        """
        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.force_delete_pipeline(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/force-delete"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_playground_session(self, pipeline_id: str) -> PlaygroundSession:
        """
        Get a playground session for a user and pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_playground_session(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/playground-session"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PlaygroundSession, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def run_search(
        self,
        pipeline_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        alpha: typing.Optional[float] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        dense_similarity_cutoff: typing.Optional[float] = OMIT,
        dense_similarity_top_k: typing.Optional[int] = OMIT,
        enable_reranking: typing.Optional[bool] = OMIT,
        files_top_k: typing.Optional[int] = OMIT,
        query: str,
        rerank_top_n: typing.Optional[int] = OMIT,
        retrieval_mode: typing.Optional[RetrievalMode] = OMIT,
        retrieve_image_nodes: typing.Optional[bool] = OMIT,
        retrieve_page_figure_nodes: typing.Optional[bool] = OMIT,
        retrieve_page_screenshot_nodes: typing.Optional[bool] = OMIT,
        search_filters: typing.Optional[MetadataFilters] = OMIT,
        search_filters_inference_schema: typing.Optional[
            typing.Dict[str, typing.Optional[RetrievalParamsSearchFiltersInferenceSchemaValue]]
        ] = OMIT,
        sparse_similarity_top_k: typing.Optional[int] = OMIT,
    ) -> RetrieveResults:
        """
        Get retrieval results for a managed pipeline and a query

        Parameters:
            - pipeline_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - alpha: typing.Optional[float].

            - class_name: typing.Optional[str].

            - dense_similarity_cutoff: typing.Optional[float].

            - dense_similarity_top_k: typing.Optional[int].

            - enable_reranking: typing.Optional[bool].

            - files_top_k: typing.Optional[int].

            - query: str. The query to retrieve against.

            - rerank_top_n: typing.Optional[int].

            - retrieval_mode: typing.Optional[RetrievalMode]. The retrieval mode for the query.

            - retrieve_image_nodes: typing.Optional[bool]. Whether to retrieve image nodes.

            - retrieve_page_figure_nodes: typing.Optional[bool]. Whether to retrieve page figure nodes.

            - retrieve_page_screenshot_nodes: typing.Optional[bool]. Whether to retrieve page screenshot nodes.

            - search_filters: typing.Optional[MetadataFilters].

            - search_filters_inference_schema: typing.Optional[typing.Dict[str, typing.Optional[RetrievalParamsSearchFiltersInferenceSchemaValue]]].

            - sparse_similarity_top_k: typing.Optional[int].
        ---
        from llama_cloud import FilterCondition, MetadataFilters, RetrievalMode
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.run_search(
            pipeline_id="string",
            query="string",
            retrieval_mode=RetrievalMode.CHUNKS,
            search_filters=MetadataFilters(
                condition=FilterCondition.AND,
                filters=[],
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {"query": query}
        if alpha is not OMIT:
            _request["alpha"] = alpha
        if class_name is not OMIT:
            _request["class_name"] = class_name
        if dense_similarity_cutoff is not OMIT:
            _request["dense_similarity_cutoff"] = dense_similarity_cutoff
        if dense_similarity_top_k is not OMIT:
            _request["dense_similarity_top_k"] = dense_similarity_top_k
        if enable_reranking is not OMIT:
            _request["enable_reranking"] = enable_reranking
        if files_top_k is not OMIT:
            _request["files_top_k"] = files_top_k
        if rerank_top_n is not OMIT:
            _request["rerank_top_n"] = rerank_top_n
        if retrieval_mode is not OMIT:
            _request["retrieval_mode"] = retrieval_mode
        if retrieve_image_nodes is not OMIT:
            _request["retrieve_image_nodes"] = retrieve_image_nodes
        if retrieve_page_figure_nodes is not OMIT:
            _request["retrieve_page_figure_nodes"] = retrieve_page_figure_nodes
        if retrieve_page_screenshot_nodes is not OMIT:
            _request["retrieve_page_screenshot_nodes"] = retrieve_page_screenshot_nodes
        if search_filters is not OMIT:
            _request["search_filters"] = search_filters
        if search_filters_inference_schema is not OMIT:
            _request["search_filters_inference_schema"] = search_filters_inference_schema
        if sparse_similarity_top_k is not OMIT:
            _request["sparse_similarity_top_k"] = sparse_similarity_top_k
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/retrieve"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(RetrieveResults, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_status(
        self, pipeline_id: str, *, full_details: typing.Optional[bool] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a pipeline by ID.

        Parameters:
            - pipeline_id: str.

            - full_details: typing.Optional[bool].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_status(
            pipeline_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/status"),
            params=remove_none_from_dict({"full_details": full_details}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def sync_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Run ingestion for the pipeline by incrementally updating the data-sink with upstream changes from data-sources & files.

        Parameters:
            - pipeline_id: str.
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/sync"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def cancel_pipeline_sync(self, pipeline_id: str) -> Pipeline:
        """
        Parameters:
            - pipeline_id: str.
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/sync/cancel"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncPipelinesClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def search_pipelines(
        self,
        *,
        project_id: typing.Optional[str] = None,
        project_name: typing.Optional[str] = None,
        pipeline_name: typing.Optional[str] = None,
        pipeline_type: typing.Optional[PipelineType] = None,
        organization_id: typing.Optional[str] = None,
    ) -> typing.List[Pipeline]:
        """
        Search for pipelines by various parameters.

        Parameters:
            - project_id: typing.Optional[str].

            - project_name: typing.Optional[str].

            - pipeline_name: typing.Optional[str].

            - pipeline_type: typing.Optional[PipelineType].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import PipelineType
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.search_pipelines(
            pipeline_type=PipelineType.PLAYGROUND,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict(
                {
                    "project_id": project_id,
                    "project_name": project_name,
                    "pipeline_name": pipeline_name,
                    "pipeline_type": pipeline_type,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[Pipeline], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_pipeline(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request: PipelineCreate,
    ) -> Pipeline:
        """
        Create a new pipeline for a project.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - request: PipelineCreate.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_pipeline(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request: PipelineCreate,
    ) -> Pipeline:
        """
        Upsert a pipeline for a project.
        Updates if a pipeline with the same name and project_id already exists. Otherwise, creates a new pipeline.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - request: PipelineCreate.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/pipelines"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Get a pipeline by ID for a given project.

        Parameters:
            - pipeline_id: str.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_existing_pipeline(
        self,
        pipeline_id: str,
        *,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        metadata_config: typing.Optional[PipelineMetadataConfig] = OMIT,
        name: typing.Optional[str] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        sparse_model_config: typing.Optional[SparseModelConfig] = OMIT,
        status: typing.Optional[str] = OMIT,
        transform_config: typing.Optional[PipelineUpdateTransformConfig] = OMIT,
    ) -> Pipeline:
        """
        Update an existing pipeline for a project.

        Parameters:
            - pipeline_id: str.

            - data_sink: typing.Optional[DataSinkCreate].

            - data_sink_id: typing.Optional[str].

            - embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig].

            - embedding_model_config_id: typing.Optional[str].

            - llama_parse_parameters: typing.Optional[LlamaParseParameters].

            - managed_pipeline_id: typing.Optional[str].

            - metadata_config: typing.Optional[PipelineMetadataConfig].

            - name: typing.Optional[str].

            - preset_retrieval_parameters: typing.Optional[PresetRetrievalParams].

            - sparse_model_config: typing.Optional[SparseModelConfig].

            - status: typing.Optional[str].

            - transform_config: typing.Optional[PipelineUpdateTransformConfig]. Configuration for the transformation.
        """
        _request: typing.Dict[str, typing.Any] = {}
        if data_sink is not OMIT:
            _request["data_sink"] = data_sink
        if data_sink_id is not OMIT:
            _request["data_sink_id"] = data_sink_id
        if embedding_config is not OMIT:
            _request["embedding_config"] = embedding_config
        if embedding_model_config_id is not OMIT:
            _request["embedding_model_config_id"] = embedding_model_config_id
        if llama_parse_parameters is not OMIT:
            _request["llama_parse_parameters"] = llama_parse_parameters
        if managed_pipeline_id is not OMIT:
            _request["managed_pipeline_id"] = managed_pipeline_id
        if metadata_config is not OMIT:
            _request["metadata_config"] = metadata_config
        if name is not OMIT:
            _request["name"] = name
        if preset_retrieval_parameters is not OMIT:
            _request["preset_retrieval_parameters"] = preset_retrieval_parameters
        if sparse_model_config is not OMIT:
            _request["sparse_model_config"] = sparse_model_config
        if status is not OMIT:
            _request["status"] = status
        if transform_config is not OMIT:
            _request["transform_config"] = transform_config
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline(self, pipeline_id: str) -> None:
        """
        Delete a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.delete_pipeline(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def chat(
        self,
        pipeline_id: str,
        *,
        class_name: typing.Optional[str] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        messages: typing.Optional[typing.List[InputMessage]] = OMIT,
    ) -> typing.Any:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters:
            - pipeline_id: str.

            - class_name: typing.Optional[str].

            - data: typing.Optional[ChatData].

            - messages: typing.Optional[typing.List[InputMessage]].
        ---
        from llama_cloud import (
            ChatData,
            FilterCondition,
            LlmParameters,
            MetadataFilters,
            PresetRetrievalParams,
            RetrievalMode,
            SupportedLlmModelNames,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.chat(
            pipeline_id="string",
            data=ChatData(
                llm_parameters=LlmParameters(
                    model_name=SupportedLlmModelNames.GPT_4_O,
                ),
                retrieval_parameters=PresetRetrievalParams(
                    retrieval_mode=RetrievalMode.CHUNKS,
                    search_filters=MetadataFilters(
                        condition=FilterCondition.AND,
                        filters=[],
                    ),
                ),
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if class_name is not OMIT:
            _request["class_name"] = class_name
        if data is not OMIT:
            _request["data"] = data
        if messages is not OMIT:
            _request["messages"] = messages
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/chat"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def copy_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Copy a pipeline by ID.

        Parameters:
            - pipeline_id: str.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/copy"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        only_api_data_source_documents: typing.Optional[bool] = None,
        status_refresh_policy: typing.Optional[
            ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy
        ] = None,
    ) -> typing.List[CloudDocument]:
        """
        Return a list of documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - skip: typing.Optional[int].

            - limit: typing.Optional[int].

            - file_id: typing.Optional[str].

            - only_direct_upload: typing.Optional[bool].

            - only_api_data_source_documents: typing.Optional[bool].

            - status_refresh_policy: typing.Optional[ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy].
        ---
        from llama_cloud import (
            ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.list_pipeline_documents(
            pipeline_id="string",
            status_refresh_policy=ListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsGetRequestStatusRefreshPolicy.CACHED,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            params=remove_none_from_dict(
                {
                    "skip": skip,
                    "limit": limit,
                    "file_id": file_id,
                    "only_direct_upload": only_direct_upload,
                    "only_api_data_source_documents": only_api_data_source_documents,
                    "status_refresh_policy": status_refresh_policy,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_batch_pipeline_documents(
        self, pipeline_id: str, *, request: typing.List[CloudDocumentCreate]
    ) -> typing.List[CloudDocument]:
        """
        Batch create documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[CloudDocumentCreate].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.create_batch_pipeline_documents(
            pipeline_id="string",
            request=[],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_batch_pipeline_documents(
        self, pipeline_id: str, *, request: typing.List[CloudDocumentCreate]
    ) -> typing.List[CloudDocument]:
        """
        Batch create or update a document for a pipeline.

        Parameters:
            - pipeline_id: str.

            - request: typing.List[CloudDocumentCreate].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.upsert_batch_pipeline_documents(
            pipeline_id="string",
            request=[],
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents"
            ),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[CloudDocument], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def paginated_list_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        only_api_data_source_documents: typing.Optional[bool] = None,
        status_refresh_policy: typing.Optional[
            PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy
        ] = None,
    ) -> PaginatedListCloudDocumentsResponse:
        """
        Return a list of documents for a pipeline.

        Parameters:
            - pipeline_id: str.

            - skip: typing.Optional[int].

            - limit: typing.Optional[int].

            - file_id: typing.Optional[str].

            - only_direct_upload: typing.Optional[bool].

            - only_api_data_source_documents: typing.Optional[bool].

            - status_refresh_policy: typing.Optional[PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy].
        ---
        from llama_cloud import (
            PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.paginated_list_pipeline_documents(
            pipeline_id="string",
            status_refresh_policy=PaginatedListPipelineDocumentsApiV1PipelinesPipelineIdDocumentsPaginatedGetRequestStatusRefreshPolicy.CACHED,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/paginated"
            ),
            params=remove_none_from_dict(
                {
                    "skip": skip,
                    "limit": limit,
                    "file_id": file_id,
                    "only_direct_upload": only_direct_upload,
                    "only_api_data_source_documents": only_api_data_source_documents,
                    "status_refresh_policy": status_refresh_policy,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedListCloudDocumentsResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_document(self, document_id: str, pipeline_id: str) -> CloudDocument:
        """
        Return a single document for a pipeline.

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_document(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/{document_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(CloudDocument, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_document(self, document_id: str, pipeline_id: str) -> None:
        """
        Delete a document from a pipeline.
        Initiates an async job that will:

        1. Delete vectors from the vector store
        2. Delete the document from MongoDB after vectors are successfully deleted

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.delete_pipeline_document(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/documents/{document_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_document_chunks(self, document_id: str, pipeline_id: str) -> typing.List[TextNode]:
        """
        Return a list of chunks for a pipeline document.

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.list_pipeline_document_chunks(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/chunks",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.List[TextNode], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_document_status(self, document_id: str, pipeline_id: str) -> ManagedIngestionStatusResponse:
        """
        Return a single document for a pipeline.

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_document_status(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/status",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def sync_pipeline_document(self, document_id: str, pipeline_id: str) -> typing.Any:
        """
        Sync a specific document for a pipeline.

        Parameters:
            - document_id: str.

            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.sync_pipeline_document(
            document_id="string",
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/pipelines/{pipeline_id}/documents/{document_id}/sync",
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def force_delete_pipeline(self, pipeline_id: str) -> None:
        """
        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.force_delete_pipeline(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/force-delete"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_playground_session(self, pipeline_id: str) -> PlaygroundSession:
        """
        Get a playground session for a user and pipeline.

        Parameters:
            - pipeline_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_playground_session(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/playground-session"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PlaygroundSession, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def run_search(
        self,
        pipeline_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        alpha: typing.Optional[float] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        dense_similarity_cutoff: typing.Optional[float] = OMIT,
        dense_similarity_top_k: typing.Optional[int] = OMIT,
        enable_reranking: typing.Optional[bool] = OMIT,
        files_top_k: typing.Optional[int] = OMIT,
        query: str,
        rerank_top_n: typing.Optional[int] = OMIT,
        retrieval_mode: typing.Optional[RetrievalMode] = OMIT,
        retrieve_image_nodes: typing.Optional[bool] = OMIT,
        retrieve_page_figure_nodes: typing.Optional[bool] = OMIT,
        retrieve_page_screenshot_nodes: typing.Optional[bool] = OMIT,
        search_filters: typing.Optional[MetadataFilters] = OMIT,
        search_filters_inference_schema: typing.Optional[
            typing.Dict[str, typing.Optional[RetrievalParamsSearchFiltersInferenceSchemaValue]]
        ] = OMIT,
        sparse_similarity_top_k: typing.Optional[int] = OMIT,
    ) -> RetrieveResults:
        """
        Get retrieval results for a managed pipeline and a query

        Parameters:
            - pipeline_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - alpha: typing.Optional[float].

            - class_name: typing.Optional[str].

            - dense_similarity_cutoff: typing.Optional[float].

            - dense_similarity_top_k: typing.Optional[int].

            - enable_reranking: typing.Optional[bool].

            - files_top_k: typing.Optional[int].

            - query: str. The query to retrieve against.

            - rerank_top_n: typing.Optional[int].

            - retrieval_mode: typing.Optional[RetrievalMode]. The retrieval mode for the query.

            - retrieve_image_nodes: typing.Optional[bool]. Whether to retrieve image nodes.

            - retrieve_page_figure_nodes: typing.Optional[bool]. Whether to retrieve page figure nodes.

            - retrieve_page_screenshot_nodes: typing.Optional[bool]. Whether to retrieve page screenshot nodes.

            - search_filters: typing.Optional[MetadataFilters].

            - search_filters_inference_schema: typing.Optional[typing.Dict[str, typing.Optional[RetrievalParamsSearchFiltersInferenceSchemaValue]]].

            - sparse_similarity_top_k: typing.Optional[int].
        ---
        from llama_cloud import FilterCondition, MetadataFilters, RetrievalMode
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.run_search(
            pipeline_id="string",
            query="string",
            retrieval_mode=RetrievalMode.CHUNKS,
            search_filters=MetadataFilters(
                condition=FilterCondition.AND,
                filters=[],
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {"query": query}
        if alpha is not OMIT:
            _request["alpha"] = alpha
        if class_name is not OMIT:
            _request["class_name"] = class_name
        if dense_similarity_cutoff is not OMIT:
            _request["dense_similarity_cutoff"] = dense_similarity_cutoff
        if dense_similarity_top_k is not OMIT:
            _request["dense_similarity_top_k"] = dense_similarity_top_k
        if enable_reranking is not OMIT:
            _request["enable_reranking"] = enable_reranking
        if files_top_k is not OMIT:
            _request["files_top_k"] = files_top_k
        if rerank_top_n is not OMIT:
            _request["rerank_top_n"] = rerank_top_n
        if retrieval_mode is not OMIT:
            _request["retrieval_mode"] = retrieval_mode
        if retrieve_image_nodes is not OMIT:
            _request["retrieve_image_nodes"] = retrieve_image_nodes
        if retrieve_page_figure_nodes is not OMIT:
            _request["retrieve_page_figure_nodes"] = retrieve_page_figure_nodes
        if retrieve_page_screenshot_nodes is not OMIT:
            _request["retrieve_page_screenshot_nodes"] = retrieve_page_screenshot_nodes
        if search_filters is not OMIT:
            _request["search_filters"] = search_filters
        if search_filters_inference_schema is not OMIT:
            _request["search_filters_inference_schema"] = search_filters_inference_schema
        if sparse_similarity_top_k is not OMIT:
            _request["sparse_similarity_top_k"] = sparse_similarity_top_k
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/retrieve"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(RetrieveResults, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_status(
        self, pipeline_id: str, *, full_details: typing.Optional[bool] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a pipeline by ID.

        Parameters:
            - pipeline_id: str.

            - full_details: typing.Optional[bool].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.pipelines.get_pipeline_status(
            pipeline_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/status"),
            params=remove_none_from_dict({"full_details": full_details}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ManagedIngestionStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def sync_pipeline(self, pipeline_id: str) -> Pipeline:
        """
        Run ingestion for the pipeline by incrementally updating the data-sink with upstream changes from data-sources & files.

        Parameters:
            - pipeline_id: str.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/sync"),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def cancel_pipeline_sync(self, pipeline_id: str) -> Pipeline:
        """
        Parameters:
            - pipeline_id: str.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/pipelines/{pipeline_id}/sync/cancel"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Pipeline, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
