# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ..core.datetime_utils import serialize_datetime
from .batch_job_type import BatchJobType
from .processing_result_job_config import ProcessingResultJobConfig
from .processing_result_metadata import ProcessingResultMetadata

try:
    import pydantic
    if pydantic.__version__.startswith("1."):
        raise ImportError
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore


class ProcessingResult(pydantic.BaseModel):
    """
    A processing result with lineage information.
    """

    item_id: str = pydantic.Field(description="Source item that was processed")
    job_config: ProcessingResultJobConfig = pydantic.Field(description="Job configuration used for processing")
    job_type: BatchJobType = pydantic.Field(description="Type of processing performed")
    output_metadata: typing.Optional[ProcessingResultMetadata]
    output_s_3_path: str = pydantic.Field(alias="output_s3_path", description="S3 location of processing output")
    parameters_hash: str = pydantic.Field(description="Hash of parameters for deduplication")
    processed_at: dt.datetime = pydantic.Field(description="When this processing occurred")
    result_id: str = pydantic.Field(description="Unique identifier for this result")

    def json(self, **kwargs: typing.Any) -> str:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().json(**kwargs_with_defaults)

    def dict(self, **kwargs: typing.Any) -> typing.Dict[str, typing.Any]:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().dict(**kwargs_with_defaults)

    class Config:
        frozen = True
        smart_union = True
        allow_population_by_field_name = True
        json_encoders = {dt.datetime: serialize_datetime}
